{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T12:57:00.011760727Z",
     "start_time": "2024-02-12T12:56:59.986387434Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022c4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86715317",
   "metadata": {},
   "source": [
    "# Experimenting with LLMs and propmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"When asked to tell a joke, only tell the joke and nothing else (no introduction, etc.).\"),\n",
    "    (\"user\", \"Tell me a joke about {input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82579a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()  # The output of LLM will be a `message` -> parse it to a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef8075",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Local LLM, download from [ollama.com](https://ollama.com/download/linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "231b03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm_llama = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedba3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why was the math book sad? Because it had too many problems! ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "response = llm_llama.invoke(\"Tell me a joke\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5e428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award? Because he was outstanding in his field! ðŸ˜„"
     ]
    }
   ],
   "source": [
    "for chunk in llm_llama.stream(\"Tell me a joke\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3642cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_llama = prompt | llm_llama | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56fa104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a joke for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs.\n"
     ]
    }
   ],
   "source": [
    "response = chain_llama.invoke({\"input\": \"programmers\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0d874cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the programmer break up with his girlfriend?\n",
      "She kept trying to fix her own problems, but he couldn't compile a solution."
     ]
    }
   ],
   "source": [
    "for chunk in chain_llama.stream({\"input\": \"programmers\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a9fcd",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Create a key (<https://platform.openai.com/api-keys>) and set it in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_openai = prompt | llm_openai | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "293a2bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? \n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "\n",
      "Tokens Used: 50\n",
      "\tPrompt Tokens: 37\n",
      "\tCompletion Tokens: 13\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $8.15e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as token_usage:\n",
    "    \n",
    "    response = chain_openai.invoke({\"input\": \"programmers\"})\n",
    "    print(response)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(token_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c955c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs.\n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as token_usage:\n",
    "    \n",
    "    for chunk in chain_openai.stream({\"input\": \"programmers\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # apparently, token counting does not work with streaming\n",
    "    print(token_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc298c",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Use Chroma vectorstore to answer questions with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a70cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML-DEECo\\n\\nML-DEECo is a machine-learning-enabled component model for adaptive component architecture'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# load the example document\n",
    "example_path = \"examples/quickstart_example_data/ML-DEECo_README.md\"\n",
    "loader = UnstructuredMarkdownLoader(example_path)\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821281c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='ML-DEECo\\n\\nML-DEECo is a machine-learning-enabled component model for adaptive component architectures. It is based on DEECo component model, which features autonomic components and dynamic component coalitions (called ensembles). ML-DEECo allows exploiting machine learning in decisions about adapting component coalitions at runtime.', metadata={'source': 'example_data/ML-DEECo_README.md'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3747b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vectorstore: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7f02182afb80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# embedding_function = OllamaEmbeddings()  # with Ollama embeddings (running on my laptop), it takes around 2 minutes to create the vectorstore\n",
    "embedding_function = OpenAIEmbeddings()  # with OpenAIEmbeddings embeddings, it takes around 10 seconds to create the vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents([docs[0]], embedding_function)\n",
    "with tqdm(total=len(docs) - 1, desc=\"Creating vectorstore\") as progress_bar:\n",
    "    for d in docs[1:]:\n",
    "        vectorstore.add_documents([d])\n",
    "        progress_bar.update(1)  \n",
    "\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c238688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a query\n",
    "query = \"What is utility?\"\n",
    "result = vectorstore.similarity_search(query)\n",
    "\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# document_chain = create_stuff_documents_chain(llm_llama, prompt)\n",
    "document_chain = create_stuff_documents_chain(llm_openai, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb4e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60791dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is utility?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents] [462ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context>] [465ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context>] [466ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\",\n",
      "  \"context\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"An ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"The framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"The ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context> > 12:chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context> > 12:chain:format_docs] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\",\n",
      "  \"context\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 13:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\",\n",
      "  \"context\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 13:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Answer the following question based only on the provided context:\\n\\n<context>\\nutility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\\n</context>\\n\\nQuestion: What is utility?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 14:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following question based only on the provided context:\\n\\n<context>\\nutility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\\n\\nAn ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\\n\\nThe framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\\n\\nThe ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\\n</context>\\n\\nQuestion: What is utility?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 14:llm:ChatOpenAI] [762ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 284,\n",
      "      \"total_tokens\": 302\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 15:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 15:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain] [767ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer>] [769ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer>] [770ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is utility?\",\n",
      "  \"context\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"utility orders the components;\\n\\ncardinality sets the maximum (or both minimum and maximum) allowed number of components to be picked.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"An ensemble is a group of components created to express that the components are bound with the same objective (i.e., behavior). The ensembles are used for knowledge exchange among the member components and their coordination. The ensembles are formed and dismantled dynamically at runtime as the objectives emerge in the system and are completed. The member components of the ensemble are also selected dynamically at every step of the simulation.\\n\\nUsage\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"The framework provides abstractions for getting predictions (estimates) about the future state of the system. It uses machine learning models trained in a supervised manner to obtain these predictions. A simulation of the system is run to collect data used for training the ML model. The simulation can then be run again with the trained model to see the impact of the learned model on the system.\\n\\nShort summary of ensemble-based component systems\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"The ML-DEECo framework provides abstractions for creating components and ensembles and assigning machine learning estimates to them. A simulation can then be run with the components and ensembles to observe behavior of the system and collect data for training the estimates. The trained estimate can then be used in the next run of the simulation.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"example_data/ML-DEECo_README.md\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"answer\": \"The provided context does not provide information about what \\\"utility\\\" refers to in this context.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain] [1.24s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\n",
      "The provided context does not provide information about what \"utility\" refers to in this context.\n"
     ]
    }
   ],
   "source": [
    "set_debug(True)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"What is utility?\"})\n",
    "\n",
    "print()\n",
    "print(response[\"answer\"])\n",
    "set_debug(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
