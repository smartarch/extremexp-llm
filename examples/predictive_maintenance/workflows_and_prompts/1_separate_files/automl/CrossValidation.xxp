package automl;

workflow TrainTestSplitValidation {
  // TASKS
  define task KFoldCrossValidationSplit; // this is a loop for generating the folds (datasets), the ML training tasks can be run in parallel
  define task MLTrainingAndEvaluation;
  define task MLModelMetricAggregation;

  // DATA
  define data InputData;
  define data Hyperparameters;

  // generated for each fold (k = 1..K)
  define data TrainingData_k;
  define data ValidationData_k;
  define data MLModelMetrics_k;

  define data AggregatedMLModelMetrics;
  define data MLModelMetrics = AggregatedMLModelMetrics; // rename output data (name AggregatedMLModelMetrics is more descriptive inside this workflow, but the parent workflow expects MLModelMetrics)

  // TASK CONNECTIONS
  
  START -> KFoldCrossValidationSplit;

  KFoldCrossValidationSplit -> MLTrainingAndEvaluation -> MLModelMetricAggregation;

  KFoldCrossValidationSplit ?-> KFoldCrossValidationSplit {
    condition "repeat K times (k = 1..K)";
  }

  KFoldCrossValidationSplit ?-> MLModelMetricAggregation {
    condition "all folds processed";
  }

  MLModelMetricAggregation -> END;

  // DATA CONNECTIONS

  // input data preprocessing and train-test split
  InputData --> KFoldCrossValidationSplit;
  KFoldCrossValidationSplit --> TrainingData_k;
  KFoldCrossValidationSplit --> ValidationData_k;

  // model training and evaluation
  Hyperparameters --> MLTrainingAndEvaluation;
  TrainingData_k --> MLTrainingAndEvaluation;
  ValidationData_k --> MLTrainingAndEvaluation --> MLModelMetrics_k;

  MLModelMetrics_k --> MLModelMetricAggregation --> AggregatedMLModelMetrics;

  // TASK CONFIGURATION
  configure task KFoldCrossValidationSplit {
    implementation "k_fold_cross_validation_split.py";
    param K = 5; // number of "folds" (generated datasets)
  }

  configure task MLTrainingAndEvaluation {
    subworkflow MLTrainingAndEvaluation;
    data TestData = ValidationData; // rename input data
  }
}
